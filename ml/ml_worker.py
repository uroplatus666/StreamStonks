import json
import os
import time
import pandas as pd
import numpy as np
import psycopg2
from psycopg2.extras import execute_values
from catboost import CatBoostRegressor
from kafka import KafkaConsumer
import logging
from datetime import datetime, timedelta

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("ML_Worker")

KAFKA_SERVER = os.getenv("KAFKA_BOOTSTRAP_SERVERS")
PG_HOST = os.getenv("POSTGRES_HOST")
PG_USER = os.getenv("POSTGRES_USER")
PG_PASS = os.getenv("POSTGRES_PASSWORD")
PG_DB = os.getenv("POSTGRES_DB")

model = CatBoostRegressor()
MODEL_PATH = "shared_models/catboost_model.cbm"

# Ð•ÑÐ»Ð¸ Ð¿Ð°Ð¿ÐºÐ¸ Ð½ÐµÑ‚ (Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð·Ð°Ð¿ÑƒÑÐº), Ð¸Ñ‰ÐµÐ¼ Ñ„Ð°Ð¹Ð» Ñ€ÑÐ´Ð¾Ð¼
if not os.path.exists(MODEL_PATH) and os.path.exists("catboost_model.cbm"):
    MODEL_PATH = "catboost_model.cbm"
last_model_load_time = 0


def load_model_if_updated():
    global last_model_load_time, model
    try:
        if not os.path.exists(MODEL_PATH): return
        mtime = os.path.getmtime(MODEL_PATH)
        if mtime > last_model_load_time:
            model.load_model(MODEL_PATH)
            last_model_load_time = mtime
            logger.info("ðŸ”„ Model reloaded (Hot Swap)")
    except Exception:
        pass


def get_db_conn():
    return psycopg2.connect(host=PG_HOST, user=PG_USER, password=PG_PASS, dbname=PG_DB)


def predict_future(ticker, horizon_hours):
    load_model_if_updated()
    conn = get_db_conn()

    # Ð‘ÐµÑ€ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 2-3 Ð´Ð½Ñ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ñ…Ð²Ð°Ñ‚Ð¸Ð»Ð¾ Ð½Ð° Ñ€Ð°ÑÑ‡ÐµÑ‚ SMA (24 Ñ‡Ð°ÑÐ°)
    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ NOW(), Ñ‚Ð°Ðº ÐºÐ°Ðº Ñƒ Ð½Ð°Ñ Live-ÑÐ¸ÑÑ‚ÐµÐ¼Ð°
    query = f"""
        SELECT ticker, close, volume, rsi, macd, day_of_week, time_of_day, ts
        FROM market_candles
        WHERE ticker = '{ticker}' AND ts > NOW() - INTERVAL '3 days'
        ORDER BY ts ASC
    """
    try:
        df = pd.read_sql(query, conn)
    except:
        conn.close();
        return []

    if len(df) < 30:
        logger.warning(f"Not enough data for {ticker}")
        conn.close();
        return []

    predictions = []

    # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ðº Ñ€ÐµÐºÑƒÑ€ÑÐ¸Ð¸
    # ÐÐ°Ð¼ Ð½ÑƒÐ¶Ð½Ð¾ Ñ€Ð°ÑÑÑ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ SMA Ð¸ Returns Ð½Ð° Ð»ÐµÑ‚Ñƒ
    # Ð‘ÐµÑ€ÐµÐ¼ Ñ…Ð²Ð¾ÑÑ‚ Ð´Ð°Ñ‚Ð°Ñ„Ñ€ÐµÐ¹Ð¼Ð° ÐºÐ°Ðº Ð±ÑƒÑ„ÐµÑ€

    current_price = df.iloc[-1]['close']
    current_ts = df.iloc[-1]['ts']

    # Ð‘ÑƒÑ„ÐµÑ€ Ð´Ð»Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° SMA (Ð½Ð°Ð¼ Ð½ÑƒÐ¶Ð½Ð¾ Ð¾ÐºÐ½Ð¾ 24)
    close_history = list(df['close'].values)

    # Ð¡Ñ‚Ð°Ñ‚Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ (ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð¸Ðµ: RSI/Volume Ð±ÐµÑ€ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ, ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ñ‡Ñ‚Ð¾ Ð½Ðµ Ð¼ÐµÐ½ÑÑŽÑ‚ÑÑ)
    last_vol = df.iloc[-1]['volume']
    last_rsi = df.iloc[-1]['rsi']
    last_macd = df.iloc[-1]['macd']

    for i in range(horizon_hours):
        next_ts = current_ts + timedelta(hours=i + 1)

        # 1. Ð¡Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ñ„Ð¸Ñ‡Ð¸ Ð½Ð° Ð»ÐµÑ‚Ñƒ
        # SMA 24
        sma_24 = np.mean(close_history[-24:])
        dist_to_sma = current_price / sma_24

        # Returns (Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ñ†ÐµÐ½Ñ‹ Ð·Ð° Ð¿Ñ€Ð¾ÑˆÐ»Ñ‹Ðµ ÑˆÐ°Ð³Ð¸)
        return_lag1 = close_history[-1] / close_history[-2]
        return_lag2 = close_history[-2] / close_history[-3]

        features = pd.DataFrame([{
            'ticker': ticker,
            'volume': last_vol,
            'rsi': last_rsi,
            'macd': last_macd,
            'day_of_week': next_ts.weekday(),
            'time_of_day': next_ts.hour,
            'return_lag1': return_lag1,
            'return_lag2': return_lag2,
            'dist_to_sma': dist_to_sma
        }])

        # 2. ÐŸÐ Ð•Ð”Ð¡ÐšÐÐ—ÐÐÐ˜Ð• (ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´Ð°ÐµÑ‚ Ratio, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ 1.002)
        pred_ratio = model.predict(features)[0]

        # 3. ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ Ð² Ð¦ÐµÐ½Ñƒ: New_Price = Old_Price * Ratio
        next_price = current_price * pred_ratio

        predictions.append({
            'ticker': ticker, 'ts': next_ts, 'price': float(next_price), 'horizon': f"{i + 1} h"
        })

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð´Ð»Ñ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ³Ð¾ ÑˆÐ°Ð³Ð°
        close_history.append(next_price)  # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð½ÑƒÑŽ Ñ†ÐµÐ½Ñƒ Ð² Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð´Ð»Ñ SMA
        current_price = next_price

    conn.close()
    return predictions


def start_consuming():
    time.sleep(15)  # Wait for Kafka
    consumer = KafkaConsumer(
        'predict_requests', bootstrap_servers=[KAFKA_SERVER],
        auto_offset_reset='latest', enable_auto_commit=True,
        group_id='ml_worker_group', value_deserializer=lambda x: json.loads(x.decode('utf-8'))
    )
    logger.info("âœ… Live Worker Started")

    for message in consumer:
        task = message.value
        try:
            start_t = time.time()
            preds = predict_future(task['ticker'], int(task['horizon']))
            dur = int((time.time() - start_t) * 1000)

            if not preds: continue

            conn = get_db_conn()
            cur = conn.cursor()

            cur.execute("DELETE FROM model_predictions WHERE ticker = %s", (task['ticker'],))

            recs = [(p['ticker'], p['ts'], p['horizon'], p['price']) for p in preds]
            execute_values(cur, "INSERT INTO model_predictions (ticker, ts, horizon, score) VALUES %s", recs)

            # Ð›Ð¾Ð³Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ Ñ†ÐµÐ½Ð°Ð¼Ð¸ (Ð´Ð»Ñ Ð¼ÐµÑ‚Ñ€Ð¸Ðº ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°)
            start_p = preds[0]['price']
            end_p = preds[-1]['price']

            cur.execute("""
                INSERT INTO prediction_logs (ticker, requested_at, horizon_hours, processing_time_ms, predicted_start_price, predicted_end_price)
                VALUES (%s, NOW(), %s, %s, %s, %s)
            """, (task['ticker'], int(task['horizon']), dur, start_p, end_p))

            conn.commit()
            conn.close()
            logger.info(f"âœ… Predicted {task['ticker']} ({dur}ms)")

        except Exception as e:
            logger.error(f"Error: {e}")


if __name__ == "__main__":
    start_consuming()